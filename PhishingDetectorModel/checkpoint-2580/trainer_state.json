{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.983071342200725,
  "eval_steps": 500,
  "global_step": 2580,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03869407496977025,
      "grad_norm": 0.310050904750824,
      "learning_rate": 0.00019999258647132646,
      "loss": 1.89,
      "step": 10
    },
    {
      "epoch": 0.0773881499395405,
      "grad_norm": 0.26102307438850403,
      "learning_rate": 0.00019997034698451395,
      "loss": 1.8695,
      "step": 20
    },
    {
      "epoch": 0.11608222490931076,
      "grad_norm": 0.29034411907196045,
      "learning_rate": 0.00019993328483702393,
      "loss": 1.838,
      "step": 30
    },
    {
      "epoch": 0.154776299879081,
      "grad_norm": 0.2583276927471161,
      "learning_rate": 0.0001998814055240823,
      "loss": 1.838,
      "step": 40
    },
    {
      "epoch": 0.19347037484885127,
      "grad_norm": 0.3152220547199249,
      "learning_rate": 0.00019981471673786452,
      "loss": 1.856,
      "step": 50
    },
    {
      "epoch": 0.23216444981862153,
      "grad_norm": 0.27075719833374023,
      "learning_rate": 0.00019973322836635518,
      "loss": 1.7595,
      "step": 60
    },
    {
      "epoch": 0.2708585247883918,
      "grad_norm": 0.2757861912250519,
      "learning_rate": 0.00019963695249188183,
      "loss": 1.8447,
      "step": 70
    },
    {
      "epoch": 0.309552599758162,
      "grad_norm": 0.2940531075000763,
      "learning_rate": 0.00019952590338932356,
      "loss": 1.7833,
      "step": 80
    },
    {
      "epoch": 0.3482466747279323,
      "grad_norm": 0.313655287027359,
      "learning_rate": 0.0001994000975239946,
      "loss": 1.8641,
      "step": 90
    },
    {
      "epoch": 0.38694074969770254,
      "grad_norm": 0.3328441083431244,
      "learning_rate": 0.00019925955354920265,
      "loss": 1.7608,
      "step": 100
    },
    {
      "epoch": 0.42563482466747277,
      "grad_norm": 0.2908269166946411,
      "learning_rate": 0.00019910429230348347,
      "loss": 1.7661,
      "step": 110
    },
    {
      "epoch": 0.46432889963724305,
      "grad_norm": 0.36207959055900574,
      "learning_rate": 0.00019893433680751103,
      "loss": 1.752,
      "step": 120
    },
    {
      "epoch": 0.5030229746070133,
      "grad_norm": 0.3106505274772644,
      "learning_rate": 0.00019874971226068415,
      "loss": 1.7864,
      "step": 130
    },
    {
      "epoch": 0.5417170495767836,
      "grad_norm": 0.31083929538726807,
      "learning_rate": 0.0001985504460373903,
      "loss": 1.7705,
      "step": 140
    },
    {
      "epoch": 0.5804111245465539,
      "grad_norm": 0.28123217821121216,
      "learning_rate": 0.00019833656768294662,
      "loss": 1.7256,
      "step": 150
    },
    {
      "epoch": 0.619105199516324,
      "grad_norm": 0.3404386341571808,
      "learning_rate": 0.00019810810890921943,
      "loss": 1.7167,
      "step": 160
    },
    {
      "epoch": 0.6577992744860943,
      "grad_norm": 0.3133604824542999,
      "learning_rate": 0.00019786510358992213,
      "loss": 1.7601,
      "step": 170
    },
    {
      "epoch": 0.6964933494558646,
      "grad_norm": 0.307573139667511,
      "learning_rate": 0.00019760758775559274,
      "loss": 1.7677,
      "step": 180
    },
    {
      "epoch": 0.7351874244256348,
      "grad_norm": 0.336857408285141,
      "learning_rate": 0.00019733559958825167,
      "loss": 1.7671,
      "step": 190
    },
    {
      "epoch": 0.7738814993954051,
      "grad_norm": 0.32655367255210876,
      "learning_rate": 0.00019704917941574051,
      "loss": 1.7819,
      "step": 200
    },
    {
      "epoch": 0.8125755743651754,
      "grad_norm": 0.3151650130748749,
      "learning_rate": 0.00019674836970574254,
      "loss": 1.7373,
      "step": 210
    },
    {
      "epoch": 0.8512696493349455,
      "grad_norm": 0.3325897455215454,
      "learning_rate": 0.00019643321505948585,
      "loss": 1.7049,
      "step": 220
    },
    {
      "epoch": 0.8899637243047158,
      "grad_norm": 0.31963473558425903,
      "learning_rate": 0.00019610376220513068,
      "loss": 1.7353,
      "step": 230
    },
    {
      "epoch": 0.9286577992744861,
      "grad_norm": 0.32919037342071533,
      "learning_rate": 0.0001957600599908406,
      "loss": 1.7194,
      "step": 240
    },
    {
      "epoch": 0.9673518742442564,
      "grad_norm": 0.3332658112049103,
      "learning_rate": 0.00019540215937754007,
      "loss": 1.7614,
      "step": 250
    },
    {
      "epoch": 1.0060459492140266,
      "grad_norm": 0.30972036719322205,
      "learning_rate": 0.00019503011343135825,
      "loss": 1.885,
      "step": 260
    },
    {
      "epoch": 1.0447400241837967,
      "grad_norm": 0.34390830993652344,
      "learning_rate": 0.00019464397731576094,
      "loss": 1.684,
      "step": 270
    },
    {
      "epoch": 1.0834340991535671,
      "grad_norm": 0.3194654881954193,
      "learning_rate": 0.00019424380828337144,
      "loss": 1.7344,
      "step": 280
    },
    {
      "epoch": 1.1221281741233373,
      "grad_norm": 0.3800148069858551,
      "learning_rate": 0.00019382966566748168,
      "loss": 1.7432,
      "step": 290
    },
    {
      "epoch": 1.1608222490931075,
      "grad_norm": 0.312924325466156,
      "learning_rate": 0.0001934016108732548,
      "loss": 1.7176,
      "step": 300
    },
    {
      "epoch": 1.199516324062878,
      "grad_norm": 0.3291662037372589,
      "learning_rate": 0.00019295970736862064,
      "loss": 1.7109,
      "step": 310
    },
    {
      "epoch": 1.238210399032648,
      "grad_norm": 0.30536580085754395,
      "learning_rate": 0.00019250402067486522,
      "loss": 1.7435,
      "step": 320
    },
    {
      "epoch": 1.2769044740024182,
      "grad_norm": 0.3609897196292877,
      "learning_rate": 0.00019203461835691594,
      "loss": 1.7034,
      "step": 330
    },
    {
      "epoch": 1.3155985489721886,
      "grad_norm": 0.2886376976966858,
      "learning_rate": 0.00019155157001332374,
      "loss": 1.653,
      "step": 340
    },
    {
      "epoch": 1.3542926239419588,
      "grad_norm": 0.30387023091316223,
      "learning_rate": 0.00019105494726594344,
      "loss": 1.7181,
      "step": 350
    },
    {
      "epoch": 1.3929866989117292,
      "grad_norm": 0.30884063243865967,
      "learning_rate": 0.00019054482374931467,
      "loss": 1.7083,
      "step": 360
    },
    {
      "epoch": 1.4316807738814994,
      "grad_norm": 0.2775707542896271,
      "learning_rate": 0.00019002127509974376,
      "loss": 1.6449,
      "step": 370
    },
    {
      "epoch": 1.4703748488512696,
      "grad_norm": 0.3655175268650055,
      "learning_rate": 0.00018948437894408918,
      "loss": 1.6952,
      "step": 380
    },
    {
      "epoch": 1.5090689238210397,
      "grad_norm": 0.3715747594833374,
      "learning_rate": 0.0001889342148882519,
      "loss": 1.644,
      "step": 390
    },
    {
      "epoch": 1.5477629987908101,
      "grad_norm": 0.33608278632164,
      "learning_rate": 0.00018837086450537193,
      "loss": 1.6899,
      "step": 400
    },
    {
      "epoch": 1.5864570737605805,
      "grad_norm": 0.35065826773643494,
      "learning_rate": 0.00018779441132373362,
      "loss": 1.6392,
      "step": 410
    },
    {
      "epoch": 1.6251511487303507,
      "grad_norm": 0.34828153252601624,
      "learning_rate": 0.00018720494081438078,
      "loss": 1.6582,
      "step": 420
    },
    {
      "epoch": 1.663845223700121,
      "grad_norm": 0.32466286420822144,
      "learning_rate": 0.00018660254037844388,
      "loss": 1.7382,
      "step": 430
    },
    {
      "epoch": 1.7025392986698913,
      "grad_norm": 0.3389889597892761,
      "learning_rate": 0.000185987299334181,
      "loss": 1.6762,
      "step": 440
    },
    {
      "epoch": 1.7412333736396615,
      "grad_norm": 0.35634416341781616,
      "learning_rate": 0.00018535930890373466,
      "loss": 1.713,
      "step": 450
    },
    {
      "epoch": 1.7799274486094316,
      "grad_norm": 0.32683029770851135,
      "learning_rate": 0.00018471866219960602,
      "loss": 1.6953,
      "step": 460
    },
    {
      "epoch": 1.818621523579202,
      "grad_norm": 0.37082967162132263,
      "learning_rate": 0.0001840654542108494,
      "loss": 1.637,
      "step": 470
    },
    {
      "epoch": 1.8573155985489722,
      "grad_norm": 0.328664630651474,
      "learning_rate": 0.0001833997817889878,
      "loss": 1.6489,
      "step": 480
    },
    {
      "epoch": 1.8960096735187424,
      "grad_norm": 0.30127376317977905,
      "learning_rate": 0.000182721743633653,
      "loss": 1.6813,
      "step": 490
    },
    {
      "epoch": 1.9347037484885128,
      "grad_norm": 0.3401409983634949,
      "learning_rate": 0.0001820314402779511,
      "loss": 1.6923,
      "step": 500
    },
    {
      "epoch": 1.973397823458283,
      "grad_norm": 0.334755003452301,
      "learning_rate": 0.00018132897407355657,
      "loss": 1.657,
      "step": 510
    },
    {
      "epoch": 2.012091898428053,
      "grad_norm": 0.3042658567428589,
      "learning_rate": 0.00018061444917553629,
      "loss": 1.74,
      "step": 520
    },
    {
      "epoch": 2.0507859733978235,
      "grad_norm": 0.30981016159057617,
      "learning_rate": 0.00017988797152690671,
      "loss": 1.6038,
      "step": 530
    },
    {
      "epoch": 2.0894800483675935,
      "grad_norm": 0.3209863007068634,
      "learning_rate": 0.00017914964884292544,
      "loss": 1.6879,
      "step": 540
    },
    {
      "epoch": 2.128174123337364,
      "grad_norm": 0.3875134587287903,
      "learning_rate": 0.00017839959059512016,
      "loss": 1.6706,
      "step": 550
    },
    {
      "epoch": 2.1668681983071343,
      "grad_norm": 0.32049277424812317,
      "learning_rate": 0.00017763790799505747,
      "loss": 1.5941,
      "step": 560
    },
    {
      "epoch": 2.2055622732769047,
      "grad_norm": 0.42775753140449524,
      "learning_rate": 0.0001768647139778532,
      "loss": 1.6078,
      "step": 570
    },
    {
      "epoch": 2.2442563482466746,
      "grad_norm": 0.3513626158237457,
      "learning_rate": 0.0001760801231854278,
      "loss": 1.6493,
      "step": 580
    },
    {
      "epoch": 2.282950423216445,
      "grad_norm": 0.31742262840270996,
      "learning_rate": 0.00017528425194950794,
      "loss": 1.6617,
      "step": 590
    },
    {
      "epoch": 2.321644498186215,
      "grad_norm": 0.4886602759361267,
      "learning_rate": 0.0001744772182743782,
      "loss": 1.6554,
      "step": 600
    },
    {
      "epoch": 2.3603385731559854,
      "grad_norm": 0.31761476397514343,
      "learning_rate": 0.0001736591418193844,
      "loss": 1.595,
      "step": 610
    },
    {
      "epoch": 2.399032648125756,
      "grad_norm": 0.337412029504776,
      "learning_rate": 0.00017283014388119159,
      "loss": 1.6491,
      "step": 620
    },
    {
      "epoch": 2.437726723095526,
      "grad_norm": 0.4288244843482971,
      "learning_rate": 0.0001719903473757996,
      "loss": 1.6359,
      "step": 630
    },
    {
      "epoch": 2.476420798065296,
      "grad_norm": 0.3910764753818512,
      "learning_rate": 0.0001711398768203178,
      "loss": 1.6961,
      "step": 640
    },
    {
      "epoch": 2.5151148730350665,
      "grad_norm": 0.3632380962371826,
      "learning_rate": 0.00017027885831450318,
      "loss": 1.6711,
      "step": 650
    },
    {
      "epoch": 2.5538089480048365,
      "grad_norm": 0.33654922246932983,
      "learning_rate": 0.0001694074195220634,
      "loss": 1.64,
      "step": 660
    },
    {
      "epoch": 2.592503022974607,
      "grad_norm": 0.3825141489505768,
      "learning_rate": 0.00016852568965172791,
      "loss": 1.6215,
      "step": 670
    },
    {
      "epoch": 2.6311970979443773,
      "grad_norm": 0.3239041566848755,
      "learning_rate": 0.00016763379943809028,
      "loss": 1.6734,
      "step": 680
    },
    {
      "epoch": 2.6698911729141477,
      "grad_norm": 0.3867935240268707,
      "learning_rate": 0.00016673188112222394,
      "loss": 1.642,
      "step": 690
    },
    {
      "epoch": 2.7085852478839176,
      "grad_norm": 0.3828476071357727,
      "learning_rate": 0.0001658200684320748,
      "loss": 1.638,
      "step": 700
    },
    {
      "epoch": 2.747279322853688,
      "grad_norm": 0.43130332231521606,
      "learning_rate": 0.00016489849656263337,
      "loss": 1.6101,
      "step": 710
    },
    {
      "epoch": 2.7859733978234584,
      "grad_norm": 0.31580793857574463,
      "learning_rate": 0.00016396730215588915,
      "loss": 1.5948,
      "step": 720
    },
    {
      "epoch": 2.8246674727932284,
      "grad_norm": 0.4578893482685089,
      "learning_rate": 0.00016302662328057088,
      "loss": 1.5648,
      "step": 730
    },
    {
      "epoch": 2.863361547762999,
      "grad_norm": 0.36220914125442505,
      "learning_rate": 0.00016207659941167485,
      "loss": 1.6464,
      "step": 740
    },
    {
      "epoch": 2.902055622732769,
      "grad_norm": 0.3201431334018707,
      "learning_rate": 0.00016111737140978494,
      "loss": 1.665,
      "step": 750
    },
    {
      "epoch": 2.940749697702539,
      "grad_norm": 0.3619796633720398,
      "learning_rate": 0.00016014908150018703,
      "loss": 1.6588,
      "step": 760
    },
    {
      "epoch": 2.9794437726723095,
      "grad_norm": 0.33127981424331665,
      "learning_rate": 0.00015917187325178138,
      "loss": 1.6452,
      "step": 770
    },
    {
      "epoch": 3.01813784764208,
      "grad_norm": 0.3624836206436157,
      "learning_rate": 0.0001581858915557953,
      "loss": 1.7529,
      "step": 780
    },
    {
      "epoch": 3.05683192261185,
      "grad_norm": 0.3670646846294403,
      "learning_rate": 0.0001571912826043003,
      "loss": 1.5673,
      "step": 790
    },
    {
      "epoch": 3.0955259975816203,
      "grad_norm": 0.33021166920661926,
      "learning_rate": 0.00015618819386853606,
      "loss": 1.6335,
      "step": 800
    },
    {
      "epoch": 3.1342200725513907,
      "grad_norm": 0.36220091581344604,
      "learning_rate": 0.0001551767740770446,
      "loss": 1.6452,
      "step": 810
    },
    {
      "epoch": 3.1729141475211606,
      "grad_norm": 0.320946604013443,
      "learning_rate": 0.00015415717319361847,
      "loss": 1.6128,
      "step": 820
    },
    {
      "epoch": 3.211608222490931,
      "grad_norm": 0.35434311628341675,
      "learning_rate": 0.00015312954239506533,
      "loss": 1.5765,
      "step": 830
    },
    {
      "epoch": 3.2503022974607014,
      "grad_norm": 0.30407479405403137,
      "learning_rate": 0.00015209403404879303,
      "loss": 1.5679,
      "step": 840
    },
    {
      "epoch": 3.2889963724304714,
      "grad_norm": 0.36708152294158936,
      "learning_rate": 0.0001510508016902179,
      "loss": 1.6433,
      "step": 850
    },
    {
      "epoch": 3.327690447400242,
      "grad_norm": 0.3559718132019043,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.582,
      "step": 860
    },
    {
      "epoch": 3.366384522370012,
      "grad_norm": 0.38705629110336304,
      "learning_rate": 0.00014894178478110857,
      "loss": 1.5856,
      "step": 870
    },
    {
      "epoch": 3.4050785973397826,
      "grad_norm": 0.4096190929412842,
      "learning_rate": 0.00014787631293572092,
      "loss": 1.5856,
      "step": 880
    },
    {
      "epoch": 3.4437726723095525,
      "grad_norm": 0.3806115686893463,
      "learning_rate": 0.0001468037424419586,
      "loss": 1.5833,
      "step": 890
    },
    {
      "epoch": 3.482466747279323,
      "grad_norm": 0.56861412525177,
      "learning_rate": 0.00014572423233046386,
      "loss": 1.5819,
      "step": 900
    },
    {
      "epoch": 3.521160822249093,
      "grad_norm": 0.40469032526016235,
      "learning_rate": 0.00014463794266081993,
      "loss": 1.6441,
      "step": 910
    },
    {
      "epoch": 3.5598548972188633,
      "grad_norm": 0.36937376856803894,
      "learning_rate": 0.00014354503449781912,
      "loss": 1.5744,
      "step": 920
    },
    {
      "epoch": 3.5985489721886337,
      "grad_norm": 0.3666711747646332,
      "learning_rate": 0.00014244566988758152,
      "loss": 1.6085,
      "step": 930
    },
    {
      "epoch": 3.637243047158404,
      "grad_norm": 0.3643125295639038,
      "learning_rate": 0.00014134001183352832,
      "loss": 1.633,
      "step": 940
    },
    {
      "epoch": 3.675937122128174,
      "grad_norm": 0.33940136432647705,
      "learning_rate": 0.00014022822427221324,
      "loss": 1.5931,
      "step": 950
    },
    {
      "epoch": 3.7146311970979444,
      "grad_norm": 0.3418519198894501,
      "learning_rate": 0.0001391104720490156,
      "loss": 1.5715,
      "step": 960
    },
    {
      "epoch": 3.7533252720677144,
      "grad_norm": 0.3717286288738251,
      "learning_rate": 0.00013798692089369855,
      "loss": 1.6312,
      "step": 970
    },
    {
      "epoch": 3.7920193470374848,
      "grad_norm": 0.38087040185928345,
      "learning_rate": 0.00013685773739583617,
      "loss": 1.5873,
      "step": 980
    },
    {
      "epoch": 3.830713422007255,
      "grad_norm": 0.6682114601135254,
      "learning_rate": 0.0001357230889801133,
      "loss": 1.6116,
      "step": 990
    },
    {
      "epoch": 3.8694074969770256,
      "grad_norm": 0.39745864272117615,
      "learning_rate": 0.00013458314388150114,
      "loss": 1.5578,
      "step": 1000
    },
    {
      "epoch": 3.9081015719467955,
      "grad_norm": 0.3766669034957886,
      "learning_rate": 0.00013343807112031327,
      "loss": 1.6051,
      "step": 1010
    },
    {
      "epoch": 3.946795646916566,
      "grad_norm": 0.34115782380104065,
      "learning_rate": 0.00013228804047714463,
      "loss": 1.5767,
      "step": 1020
    },
    {
      "epoch": 3.985489721886336,
      "grad_norm": 0.3302670121192932,
      "learning_rate": 0.00013113322246769817,
      "loss": 1.5892,
      "step": 1030
    },
    {
      "epoch": 4.024183796856106,
      "grad_norm": 0.43377140164375305,
      "learning_rate": 0.00012997378831750242,
      "loss": 1.6618,
      "step": 1040
    },
    {
      "epoch": 4.062877871825877,
      "grad_norm": 0.3821198046207428,
      "learning_rate": 0.00012880990993652377,
      "loss": 1.567,
      "step": 1050
    },
    {
      "epoch": 4.101571946795647,
      "grad_norm": 0.3919064700603485,
      "learning_rate": 0.00012764175989367718,
      "loss": 1.5365,
      "step": 1060
    },
    {
      "epoch": 4.1402660217654175,
      "grad_norm": 0.3760301470756531,
      "learning_rate": 0.00012646951139123934,
      "loss": 1.5665,
      "step": 1070
    },
    {
      "epoch": 4.178960096735187,
      "grad_norm": 0.3724341094493866,
      "learning_rate": 0.00012529333823916807,
      "loss": 1.6284,
      "step": 1080
    },
    {
      "epoch": 4.217654171704957,
      "grad_norm": 0.41644522547721863,
      "learning_rate": 0.0001241134148293311,
      "loss": 1.5478,
      "step": 1090
    },
    {
      "epoch": 4.256348246674728,
      "grad_norm": 0.35863786935806274,
      "learning_rate": 0.00012292991610964903,
      "loss": 1.5302,
      "step": 1100
    },
    {
      "epoch": 4.295042321644498,
      "grad_norm": 0.3559032380580902,
      "learning_rate": 0.00012174301755815571,
      "loss": 1.5999,
      "step": 1110
    },
    {
      "epoch": 4.333736396614269,
      "grad_norm": 0.3474714159965515,
      "learning_rate": 0.00012055289515698007,
      "loss": 1.5918,
      "step": 1120
    },
    {
      "epoch": 4.372430471584039,
      "grad_norm": 0.41424599289894104,
      "learning_rate": 0.00011935972536625302,
      "loss": 1.5512,
      "step": 1130
    },
    {
      "epoch": 4.411124546553809,
      "grad_norm": 0.3812865912914276,
      "learning_rate": 0.00011816368509794364,
      "loss": 1.5593,
      "step": 1140
    },
    {
      "epoch": 4.449818621523579,
      "grad_norm": 0.4189375340938568,
      "learning_rate": 0.00011696495168962847,
      "loss": 1.5587,
      "step": 1150
    },
    {
      "epoch": 4.488512696493349,
      "grad_norm": 0.4319785535335541,
      "learning_rate": 0.00011576370287819736,
      "loss": 1.5856,
      "step": 1160
    },
    {
      "epoch": 4.52720677146312,
      "grad_norm": 0.3381901979446411,
      "learning_rate": 0.00011456011677350051,
      "loss": 1.5578,
      "step": 1170
    },
    {
      "epoch": 4.56590084643289,
      "grad_norm": 0.352199524641037,
      "learning_rate": 0.0001133543718319398,
      "loss": 1.5348,
      "step": 1180
    },
    {
      "epoch": 4.6045949214026605,
      "grad_norm": 0.36044827103614807,
      "learning_rate": 0.00011214664683000927,
      "loss": 1.5674,
      "step": 1190
    },
    {
      "epoch": 4.64328899637243,
      "grad_norm": 0.3608081042766571,
      "learning_rate": 0.00011093712083778746,
      "loss": 1.5773,
      "step": 1200
    },
    {
      "epoch": 4.6819830713422,
      "grad_norm": 0.40270769596099854,
      "learning_rate": 0.0001097259731923869,
      "loss": 1.5697,
      "step": 1210
    },
    {
      "epoch": 4.720677146311971,
      "grad_norm": 0.44245612621307373,
      "learning_rate": 0.00010851338347136357,
      "loss": 1.5845,
      "step": 1220
    },
    {
      "epoch": 4.759371221281741,
      "grad_norm": 0.3418751657009125,
      "learning_rate": 0.00010729953146609076,
      "loss": 1.6058,
      "step": 1230
    },
    {
      "epoch": 4.798065296251512,
      "grad_norm": 0.4474699795246124,
      "learning_rate": 0.00010608459715510139,
      "loss": 1.545,
      "step": 1240
    },
    {
      "epoch": 4.836759371221282,
      "grad_norm": 0.419752836227417,
      "learning_rate": 0.00010486876067740252,
      "loss": 1.6164,
      "step": 1250
    },
    {
      "epoch": 4.875453446191052,
      "grad_norm": 0.42674314975738525,
      "learning_rate": 0.0001036522023057659,
      "loss": 1.6239,
      "step": 1260
    },
    {
      "epoch": 4.914147521160822,
      "grad_norm": 0.36412885785102844,
      "learning_rate": 0.00010243510241999899,
      "loss": 1.5303,
      "step": 1270
    },
    {
      "epoch": 4.952841596130592,
      "grad_norm": 0.42176657915115356,
      "learning_rate": 0.00010121764148019976,
      "loss": 1.5455,
      "step": 1280
    },
    {
      "epoch": 4.991535671100363,
      "grad_norm": 0.34778308868408203,
      "learning_rate": 0.0001,
      "loss": 1.5679,
      "step": 1290
    },
    {
      "epoch": 5.030229746070133,
      "grad_norm": 0.3454209864139557,
      "learning_rate": 9.878235851980025e-05,
      "loss": 1.6444,
      "step": 1300
    },
    {
      "epoch": 5.0689238210399035,
      "grad_norm": 0.5100001096725464,
      "learning_rate": 9.756489758000105e-05,
      "loss": 1.5469,
      "step": 1310
    },
    {
      "epoch": 5.107617896009674,
      "grad_norm": 0.40058624744415283,
      "learning_rate": 9.63477976942341e-05,
      "loss": 1.5302,
      "step": 1320
    },
    {
      "epoch": 5.146311970979443,
      "grad_norm": 0.43048420548439026,
      "learning_rate": 9.513123932259751e-05,
      "loss": 1.5273,
      "step": 1330
    },
    {
      "epoch": 5.185006045949214,
      "grad_norm": 0.37775009870529175,
      "learning_rate": 9.391540284489862e-05,
      "loss": 1.5026,
      "step": 1340
    },
    {
      "epoch": 5.223700120918984,
      "grad_norm": 0.4565446972846985,
      "learning_rate": 9.270046853390925e-05,
      "loss": 1.5647,
      "step": 1350
    },
    {
      "epoch": 5.262394195888755,
      "grad_norm": 0.39462944865226746,
      "learning_rate": 9.148661652863642e-05,
      "loss": 1.5361,
      "step": 1360
    },
    {
      "epoch": 5.301088270858525,
      "grad_norm": 0.3583666980266571,
      "learning_rate": 9.027402680761309e-05,
      "loss": 1.5711,
      "step": 1370
    },
    {
      "epoch": 5.339782345828295,
      "grad_norm": 0.41926249861717224,
      "learning_rate": 8.906287916221259e-05,
      "loss": 1.4847,
      "step": 1380
    },
    {
      "epoch": 5.378476420798066,
      "grad_norm": 0.4438458979129791,
      "learning_rate": 8.785335316999078e-05,
      "loss": 1.5545,
      "step": 1390
    },
    {
      "epoch": 5.417170495767835,
      "grad_norm": 0.3371482193470001,
      "learning_rate": 8.664562816806022e-05,
      "loss": 1.5554,
      "step": 1400
    },
    {
      "epoch": 5.455864570737606,
      "grad_norm": 0.40250033140182495,
      "learning_rate": 8.543988322649954e-05,
      "loss": 1.5614,
      "step": 1410
    },
    {
      "epoch": 5.494558645707376,
      "grad_norm": 0.34743431210517883,
      "learning_rate": 8.423629712180265e-05,
      "loss": 1.5865,
      "step": 1420
    },
    {
      "epoch": 5.5332527206771465,
      "grad_norm": 0.4023534953594208,
      "learning_rate": 8.303504831037154e-05,
      "loss": 1.5761,
      "step": 1430
    },
    {
      "epoch": 5.571946795646917,
      "grad_norm": 0.40135055780410767,
      "learning_rate": 8.183631490205637e-05,
      "loss": 1.5911,
      "step": 1440
    },
    {
      "epoch": 5.610640870616686,
      "grad_norm": 0.3890388309955597,
      "learning_rate": 8.064027463374702e-05,
      "loss": 1.569,
      "step": 1450
    },
    {
      "epoch": 5.649334945586457,
      "grad_norm": 0.3851093351840973,
      "learning_rate": 7.944710484301995e-05,
      "loss": 1.5656,
      "step": 1460
    },
    {
      "epoch": 5.688029020556227,
      "grad_norm": 0.4909055531024933,
      "learning_rate": 7.825698244184431e-05,
      "loss": 1.5205,
      "step": 1470
    },
    {
      "epoch": 5.726723095525998,
      "grad_norm": 0.4462473690509796,
      "learning_rate": 7.707008389035101e-05,
      "loss": 1.5422,
      "step": 1480
    },
    {
      "epoch": 5.765417170495768,
      "grad_norm": 0.40349143743515015,
      "learning_rate": 7.588658517066892e-05,
      "loss": 1.5457,
      "step": 1490
    },
    {
      "epoch": 5.804111245465538,
      "grad_norm": 0.42956146597862244,
      "learning_rate": 7.470666176083192e-05,
      "loss": 1.5736,
      "step": 1500
    },
    {
      "epoch": 5.842805320435309,
      "grad_norm": 0.3850192129611969,
      "learning_rate": 7.353048860876064e-05,
      "loss": 1.553,
      "step": 1510
    },
    {
      "epoch": 5.881499395405078,
      "grad_norm": 0.3808140754699707,
      "learning_rate": 7.235824010632283e-05,
      "loss": 1.5321,
      "step": 1520
    },
    {
      "epoch": 5.920193470374849,
      "grad_norm": 0.4473516047000885,
      "learning_rate": 7.119009006347625e-05,
      "loss": 1.5589,
      "step": 1530
    },
    {
      "epoch": 5.958887545344619,
      "grad_norm": 0.36920300126075745,
      "learning_rate": 7.002621168249759e-05,
      "loss": 1.5882,
      "step": 1540
    },
    {
      "epoch": 5.9975816203143895,
      "grad_norm": 0.39162835478782654,
      "learning_rate": 6.886677753230184e-05,
      "loss": 1.4884,
      "step": 1550
    },
    {
      "epoch": 6.03627569528416,
      "grad_norm": 0.3853440284729004,
      "learning_rate": 6.77119595228554e-05,
      "loss": 1.5907,
      "step": 1560
    },
    {
      "epoch": 6.07496977025393,
      "grad_norm": 0.3913786709308624,
      "learning_rate": 6.656192887968675e-05,
      "loss": 1.5707,
      "step": 1570
    },
    {
      "epoch": 6.1136638452237,
      "grad_norm": 0.40810298919677734,
      "learning_rate": 6.541685611849887e-05,
      "loss": 1.5876,
      "step": 1580
    },
    {
      "epoch": 6.15235792019347,
      "grad_norm": 0.4531141221523285,
      "learning_rate": 6.427691101988673e-05,
      "loss": 1.5209,
      "step": 1590
    },
    {
      "epoch": 6.191051995163241,
      "grad_norm": 0.4621525704860687,
      "learning_rate": 6.314226260416382e-05,
      "loss": 1.4968,
      "step": 1600
    },
    {
      "epoch": 6.229746070133011,
      "grad_norm": 0.43148067593574524,
      "learning_rate": 6.201307910630146e-05,
      "loss": 1.5255,
      "step": 1610
    },
    {
      "epoch": 6.268440145102781,
      "grad_norm": 0.3955647051334381,
      "learning_rate": 6.0889527950984416e-05,
      "loss": 1.4968,
      "step": 1620
    },
    {
      "epoch": 6.307134220072552,
      "grad_norm": 0.387154221534729,
      "learning_rate": 5.977177572778678e-05,
      "loss": 1.5309,
      "step": 1630
    },
    {
      "epoch": 6.345828295042321,
      "grad_norm": 0.43824052810668945,
      "learning_rate": 5.865998816647171e-05,
      "loss": 1.5206,
      "step": 1640
    },
    {
      "epoch": 6.384522370012092,
      "grad_norm": 0.4388386011123657,
      "learning_rate": 5.755433011241851e-05,
      "loss": 1.537,
      "step": 1650
    },
    {
      "epoch": 6.423216444981862,
      "grad_norm": 0.3895430564880371,
      "learning_rate": 5.645496550218089e-05,
      "loss": 1.5244,
      "step": 1660
    },
    {
      "epoch": 6.4619105199516325,
      "grad_norm": 0.3660721480846405,
      "learning_rate": 5.536205733918007e-05,
      "loss": 1.5303,
      "step": 1670
    },
    {
      "epoch": 6.500604594921403,
      "grad_norm": 0.39672422409057617,
      "learning_rate": 5.4275767669536146e-05,
      "loss": 1.5853,
      "step": 1680
    },
    {
      "epoch": 6.539298669891173,
      "grad_norm": 0.34078478813171387,
      "learning_rate": 5.3196257558041386e-05,
      "loss": 1.4993,
      "step": 1690
    },
    {
      "epoch": 6.577992744860943,
      "grad_norm": 0.42835476994514465,
      "learning_rate": 5.212368706427912e-05,
      "loss": 1.5819,
      "step": 1700
    },
    {
      "epoch": 6.616686819830713,
      "grad_norm": 0.43103447556495667,
      "learning_rate": 5.105821521889147e-05,
      "loss": 1.5415,
      "step": 1710
    },
    {
      "epoch": 6.655380894800484,
      "grad_norm": 0.3736547529697418,
      "learning_rate": 5.000000000000002e-05,
      "loss": 1.5137,
      "step": 1720
    },
    {
      "epoch": 6.694074969770254,
      "grad_norm": 0.42157217860221863,
      "learning_rate": 4.894919830978212e-05,
      "loss": 1.5405,
      "step": 1730
    },
    {
      "epoch": 6.732769044740024,
      "grad_norm": 0.4281344711780548,
      "learning_rate": 4.790596595120699e-05,
      "loss": 1.5186,
      "step": 1740
    },
    {
      "epoch": 6.771463119709795,
      "grad_norm": 0.4416441023349762,
      "learning_rate": 4.687045760493468e-05,
      "loss": 1.5089,
      "step": 1750
    },
    {
      "epoch": 6.810157194679565,
      "grad_norm": 0.3907949924468994,
      "learning_rate": 4.5842826806381544e-05,
      "loss": 1.4685,
      "step": 1760
    },
    {
      "epoch": 6.848851269649335,
      "grad_norm": 0.3847430348396301,
      "learning_rate": 4.48232259229554e-05,
      "loss": 1.4998,
      "step": 1770
    },
    {
      "epoch": 6.887545344619105,
      "grad_norm": 0.4461142420768738,
      "learning_rate": 4.381180613146395e-05,
      "loss": 1.5516,
      "step": 1780
    },
    {
      "epoch": 6.9262394195888755,
      "grad_norm": 0.3823210597038269,
      "learning_rate": 4.280871739569972e-05,
      "loss": 1.5496,
      "step": 1790
    },
    {
      "epoch": 6.964933494558646,
      "grad_norm": 0.3812657594680786,
      "learning_rate": 4.181410844420474e-05,
      "loss": 1.5733,
      "step": 1800
    },
    {
      "epoch": 7.003627569528416,
      "grad_norm": 0.4502502977848053,
      "learning_rate": 4.0828126748218654e-05,
      "loss": 1.6316,
      "step": 1810
    },
    {
      "epoch": 7.042321644498187,
      "grad_norm": 0.4340777099132538,
      "learning_rate": 3.985091849981297e-05,
      "loss": 1.4987,
      "step": 1820
    },
    {
      "epoch": 7.081015719467956,
      "grad_norm": 0.3750959634780884,
      "learning_rate": 3.8882628590215074e-05,
      "loss": 1.557,
      "step": 1830
    },
    {
      "epoch": 7.119709794437727,
      "grad_norm": 0.38846877217292786,
      "learning_rate": 3.7923400588325155e-05,
      "loss": 1.5186,
      "step": 1840
    },
    {
      "epoch": 7.158403869407497,
      "grad_norm": 0.46431708335876465,
      "learning_rate": 3.697337671942913e-05,
      "loss": 1.4786,
      "step": 1850
    },
    {
      "epoch": 7.197097944377267,
      "grad_norm": 0.42426833510398865,
      "learning_rate": 3.60326978441109e-05,
      "loss": 1.5142,
      "step": 1860
    },
    {
      "epoch": 7.235792019347038,
      "grad_norm": 0.4196888208389282,
      "learning_rate": 3.510150343736668e-05,
      "loss": 1.5078,
      "step": 1870
    },
    {
      "epoch": 7.274486094316808,
      "grad_norm": 0.40166279673576355,
      "learning_rate": 3.4179931567925216e-05,
      "loss": 1.5774,
      "step": 1880
    },
    {
      "epoch": 7.313180169286578,
      "grad_norm": 0.3977965712547302,
      "learning_rate": 3.3268118877776066e-05,
      "loss": 1.5366,
      "step": 1890
    },
    {
      "epoch": 7.351874244256348,
      "grad_norm": 0.456936240196228,
      "learning_rate": 3.236620056190972e-05,
      "loss": 1.5334,
      "step": 1900
    },
    {
      "epoch": 7.3905683192261185,
      "grad_norm": 0.39164552092552185,
      "learning_rate": 3.147431034827208e-05,
      "loss": 1.5564,
      "step": 1910
    },
    {
      "epoch": 7.429262394195889,
      "grad_norm": 0.3983584940433502,
      "learning_rate": 3.059258047793661e-05,
      "loss": 1.4797,
      "step": 1920
    },
    {
      "epoch": 7.467956469165659,
      "grad_norm": 0.4075760245323181,
      "learning_rate": 2.9721141685496823e-05,
      "loss": 1.5452,
      "step": 1930
    },
    {
      "epoch": 7.50665054413543,
      "grad_norm": 0.4133293330669403,
      "learning_rate": 2.8860123179682242e-05,
      "loss": 1.4712,
      "step": 1940
    },
    {
      "epoch": 7.545344619105199,
      "grad_norm": 0.39210543036460876,
      "learning_rate": 2.800965262420043e-05,
      "loss": 1.5611,
      "step": 1950
    },
    {
      "epoch": 7.5840386940749696,
      "grad_norm": 0.4133801758289337,
      "learning_rate": 2.716985611880841e-05,
      "loss": 1.5307,
      "step": 1960
    },
    {
      "epoch": 7.62273276904474,
      "grad_norm": 0.3790527582168579,
      "learning_rate": 2.6340858180615646e-05,
      "loss": 1.5399,
      "step": 1970
    },
    {
      "epoch": 7.66142684401451,
      "grad_norm": 0.4542044699192047,
      "learning_rate": 2.5522781725621813e-05,
      "loss": 1.5398,
      "step": 1980
    },
    {
      "epoch": 7.700120918984281,
      "grad_norm": 0.3923864960670471,
      "learning_rate": 2.471574805049206e-05,
      "loss": 1.5316,
      "step": 1990
    },
    {
      "epoch": 7.738814993954051,
      "grad_norm": 0.4052368402481079,
      "learning_rate": 2.3919876814572194e-05,
      "loss": 1.5797,
      "step": 2000
    },
    {
      "epoch": 7.777509068923821,
      "grad_norm": 0.4625103771686554,
      "learning_rate": 2.3135286022146785e-05,
      "loss": 1.4654,
      "step": 2010
    },
    {
      "epoch": 7.816203143893591,
      "grad_norm": 0.37226563692092896,
      "learning_rate": 2.236209200494258e-05,
      "loss": 1.4732,
      "step": 2020
    },
    {
      "epoch": 7.8548972188633615,
      "grad_norm": 0.3968013823032379,
      "learning_rate": 2.1600409404879874e-05,
      "loss": 1.532,
      "step": 2030
    },
    {
      "epoch": 7.893591293833132,
      "grad_norm": 0.3923494815826416,
      "learning_rate": 2.0850351157074598e-05,
      "loss": 1.4601,
      "step": 2040
    },
    {
      "epoch": 7.932285368802902,
      "grad_norm": 0.3561989963054657,
      "learning_rate": 2.011202847309329e-05,
      "loss": 1.5645,
      "step": 2050
    },
    {
      "epoch": 7.970979443772673,
      "grad_norm": 0.4646661877632141,
      "learning_rate": 1.9385550824463727e-05,
      "loss": 1.5242,
      "step": 2060
    },
    {
      "epoch": 8.009673518742442,
      "grad_norm": 0.3876621425151825,
      "learning_rate": 1.8671025926443465e-05,
      "loss": 1.6212,
      "step": 2070
    },
    {
      "epoch": 8.048367593712213,
      "grad_norm": 0.3435215950012207,
      "learning_rate": 1.7968559722048906e-05,
      "loss": 1.4783,
      "step": 2080
    },
    {
      "epoch": 8.087061668681983,
      "grad_norm": 0.3962980806827545,
      "learning_rate": 1.7278256366347035e-05,
      "loss": 1.5933,
      "step": 2090
    },
    {
      "epoch": 8.125755743651753,
      "grad_norm": 0.48957642912864685,
      "learning_rate": 1.660021821101222e-05,
      "loss": 1.5355,
      "step": 2100
    },
    {
      "epoch": 8.164449818621524,
      "grad_norm": 0.43799924850463867,
      "learning_rate": 1.5934545789150623e-05,
      "loss": 1.5227,
      "step": 2110
    },
    {
      "epoch": 8.203143893591294,
      "grad_norm": 0.42237040400505066,
      "learning_rate": 1.5281337800393968e-05,
      "loss": 1.5626,
      "step": 2120
    },
    {
      "epoch": 8.241837968561065,
      "grad_norm": 0.3637399673461914,
      "learning_rate": 1.4640691096265358e-05,
      "loss": 1.4657,
      "step": 2130
    },
    {
      "epoch": 8.280532043530835,
      "grad_norm": 0.3912281095981598,
      "learning_rate": 1.401270066581899e-05,
      "loss": 1.5457,
      "step": 2140
    },
    {
      "epoch": 8.319226118500605,
      "grad_norm": 0.4158885180950165,
      "learning_rate": 1.339745962155613e-05,
      "loss": 1.5037,
      "step": 2150
    },
    {
      "epoch": 8.357920193470374,
      "grad_norm": 0.385614275932312,
      "learning_rate": 1.2795059185619229e-05,
      "loss": 1.5294,
      "step": 2160
    },
    {
      "epoch": 8.396614268440144,
      "grad_norm": 0.37818288803100586,
      "learning_rate": 1.2205588676266388e-05,
      "loss": 1.5172,
      "step": 2170
    },
    {
      "epoch": 8.435308343409915,
      "grad_norm": 0.40491095185279846,
      "learning_rate": 1.1629135494628096e-05,
      "loss": 1.5393,
      "step": 2180
    },
    {
      "epoch": 8.474002418379685,
      "grad_norm": 0.3933706283569336,
      "learning_rate": 1.1065785111748117e-05,
      "loss": 1.5239,
      "step": 2190
    },
    {
      "epoch": 8.512696493349456,
      "grad_norm": 0.4098847508430481,
      "learning_rate": 1.0515621055910817e-05,
      "loss": 1.4819,
      "step": 2200
    },
    {
      "epoch": 8.551390568319226,
      "grad_norm": 0.3577728867530823,
      "learning_rate": 9.978724900256265e-06,
      "loss": 1.5621,
      "step": 2210
    },
    {
      "epoch": 8.590084643288996,
      "grad_norm": 0.38918009400367737,
      "learning_rate": 9.455176250685338e-06,
      "loss": 1.5151,
      "step": 2220
    },
    {
      "epoch": 8.628778718258767,
      "grad_norm": 0.40068551898002625,
      "learning_rate": 8.945052734056581e-06,
      "loss": 1.4792,
      "step": 2230
    },
    {
      "epoch": 8.667472793228537,
      "grad_norm": 0.43755626678466797,
      "learning_rate": 8.448429986676298e-06,
      "loss": 1.4946,
      "step": 2240
    },
    {
      "epoch": 8.706166868198308,
      "grad_norm": 0.4541310667991638,
      "learning_rate": 7.96538164308407e-06,
      "loss": 1.5375,
      "step": 2250
    },
    {
      "epoch": 8.744860943168078,
      "grad_norm": 0.4059559404850006,
      "learning_rate": 7.4959793251348055e-06,
      "loss": 1.5136,
      "step": 2260
    },
    {
      "epoch": 8.783555018137848,
      "grad_norm": 0.39937981963157654,
      "learning_rate": 7.040292631379386e-06,
      "loss": 1.4467,
      "step": 2270
    },
    {
      "epoch": 8.822249093107619,
      "grad_norm": 0.3745509088039398,
      "learning_rate": 6.598389126745208e-06,
      "loss": 1.5069,
      "step": 2280
    },
    {
      "epoch": 8.860943168077387,
      "grad_norm": 0.3968782424926758,
      "learning_rate": 6.170334332518324e-06,
      "loss": 1.4904,
      "step": 2290
    },
    {
      "epoch": 8.899637243047158,
      "grad_norm": 0.4328189790248871,
      "learning_rate": 5.756191716628556e-06,
      "loss": 1.5162,
      "step": 2300
    },
    {
      "epoch": 8.938331318016928,
      "grad_norm": 0.394469290971756,
      "learning_rate": 5.3560226842390596e-06,
      "loss": 1.4701,
      "step": 2310
    },
    {
      "epoch": 8.977025392986699,
      "grad_norm": 0.4298240542411804,
      "learning_rate": 4.969886568641757e-06,
      "loss": 1.502,
      "step": 2320
    },
    {
      "epoch": 9.015719467956469,
      "grad_norm": 0.4053884446620941,
      "learning_rate": 4.597840622459937e-06,
      "loss": 1.6112,
      "step": 2330
    },
    {
      "epoch": 9.05441354292624,
      "grad_norm": 0.42148736119270325,
      "learning_rate": 4.2399400091594154e-06,
      "loss": 1.5036,
      "step": 2340
    },
    {
      "epoch": 9.09310761789601,
      "grad_norm": 0.4104529321193695,
      "learning_rate": 3.896237794869339e-06,
      "loss": 1.4793,
      "step": 2350
    },
    {
      "epoch": 9.13180169286578,
      "grad_norm": 0.4259090721607208,
      "learning_rate": 3.566784940514145e-06,
      "loss": 1.5114,
      "step": 2360
    },
    {
      "epoch": 9.17049576783555,
      "grad_norm": 0.4308828115463257,
      "learning_rate": 3.2516302942574793e-06,
      "loss": 1.5401,
      "step": 2370
    },
    {
      "epoch": 9.209189842805321,
      "grad_norm": 0.3761956989765167,
      "learning_rate": 2.9508205842594728e-06,
      "loss": 1.5433,
      "step": 2380
    },
    {
      "epoch": 9.247883917775091,
      "grad_norm": 0.40403884649276733,
      "learning_rate": 2.6644004117483356e-06,
      "loss": 1.5094,
      "step": 2390
    },
    {
      "epoch": 9.286577992744862,
      "grad_norm": 0.38598763942718506,
      "learning_rate": 2.392412244407294e-06,
      "loss": 1.5521,
      "step": 2400
    },
    {
      "epoch": 9.32527206771463,
      "grad_norm": 0.4081808626651764,
      "learning_rate": 2.134896410077891e-06,
      "loss": 1.4809,
      "step": 2410
    },
    {
      "epoch": 9.3639661426844,
      "grad_norm": 0.40228286385536194,
      "learning_rate": 1.8918910907805732e-06,
      "loss": 1.5368,
      "step": 2420
    },
    {
      "epoch": 9.402660217654171,
      "grad_norm": 0.40181535482406616,
      "learning_rate": 1.6634323170533928e-06,
      "loss": 1.531,
      "step": 2430
    },
    {
      "epoch": 9.441354292623942,
      "grad_norm": 0.37899214029312134,
      "learning_rate": 1.4495539626097288e-06,
      "loss": 1.5034,
      "step": 2440
    },
    {
      "epoch": 9.480048367593712,
      "grad_norm": 0.41820189356803894,
      "learning_rate": 1.2502877393158586e-06,
      "loss": 1.4944,
      "step": 2450
    },
    {
      "epoch": 9.518742442563482,
      "grad_norm": 0.4292626976966858,
      "learning_rate": 1.0656631924889749e-06,
      "loss": 1.5109,
      "step": 2460
    },
    {
      "epoch": 9.557436517533253,
      "grad_norm": 0.47004154324531555,
      "learning_rate": 8.957076965165235e-07,
      "loss": 1.4986,
      "step": 2470
    },
    {
      "epoch": 9.596130592503023,
      "grad_norm": 0.4069500267505646,
      "learning_rate": 7.404464507973608e-07,
      "loss": 1.5254,
      "step": 2480
    },
    {
      "epoch": 9.634824667472794,
      "grad_norm": 0.4278048574924469,
      "learning_rate": 5.999024760054095e-07,
      "loss": 1.5085,
      "step": 2490
    },
    {
      "epoch": 9.673518742442564,
      "grad_norm": 0.3820379078388214,
      "learning_rate": 4.7409661067642217e-07,
      "loss": 1.5261,
      "step": 2500
    },
    {
      "epoch": 9.712212817412334,
      "grad_norm": 0.3721475899219513,
      "learning_rate": 3.630475081181861e-07,
      "loss": 1.5095,
      "step": 2510
    },
    {
      "epoch": 9.750906892382105,
      "grad_norm": 0.3960344195365906,
      "learning_rate": 2.667716336448356e-07,
      "loss": 1.5307,
      "step": 2520
    },
    {
      "epoch": 9.789600967351873,
      "grad_norm": 0.3833034336566925,
      "learning_rate": 1.8528326213548274e-07,
      "loss": 1.5031,
      "step": 2530
    },
    {
      "epoch": 9.828295042321644,
      "grad_norm": 0.49777957797050476,
      "learning_rate": 1.1859447591769934e-07,
      "loss": 1.4964,
      "step": 2540
    },
    {
      "epoch": 9.866989117291414,
      "grad_norm": 0.37812694907188416,
      "learning_rate": 6.671516297606095e-08,
      "loss": 1.4842,
      "step": 2550
    },
    {
      "epoch": 9.905683192261185,
      "grad_norm": 0.38082632422447205,
      "learning_rate": 2.965301548606414e-08,
      "loss": 1.4986,
      "step": 2560
    },
    {
      "epoch": 9.944377267230955,
      "grad_norm": 0.37486717104911804,
      "learning_rate": 7.413528673549941e-09,
      "loss": 1.5282,
      "step": 2570
    },
    {
      "epoch": 9.983071342200725,
      "grad_norm": 0.35448887944221497,
      "learning_rate": 0.0,
      "loss": 1.5167,
      "step": 2580
    }
  ],
  "logging_steps": 10,
  "max_steps": 2580,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.752908677160247e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
